{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade -q accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "sw88JsXu1pTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers.git"
      ],
      "metadata": {
        "id": "HGkoY4-k1tHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor, LlavaForConditionalGeneration, BitsAndBytesConfig\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "SdNal7Oc5O4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageDescriptionGenerator:\n",
        "    \"\"\"\n",
        "    Class to generate textual descriptions from images using a pre-trained model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_id):\n",
        "        \"\"\"\n",
        "        Initialize the ImageDescriptionGenerator with the specified model ID.\n",
        "\n",
        "        Args:\n",
        "            model_id (str): The identifier of the model to use.\n",
        "        \"\"\"\n",
        "        self.model_id = model_id\n",
        "        self.quantization_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=torch.float16\n",
        "        )\n",
        "        self.processor = None\n",
        "        self.model = None\n",
        "        self.pipe = None\n",
        "\n",
        "    def load_model(self):\n",
        "        \"\"\"\n",
        "        Load the model and processor from the specified model ID.\n",
        "\n",
        "        Returns:\n",
        "            AutoProcessor: The processor for the model.\n",
        "            LlavaForConditionalGeneration: The loaded model.\n",
        "        \"\"\"\n",
        "        self.pipe = pipeline(\"image-to-text\", model=self.model_id, model_kwargs={\"quantization_config\": self.quantization_config})\n",
        "\n",
        "    def generate_description(self, image_path):\n",
        "        \"\"\"\n",
        "        Generate a textual description of the image using the pre-trained model.\n",
        "\n",
        "        Args:\n",
        "            image_path (str): The path to the image file.\n",
        "\n",
        "        Returns:\n",
        "            str: The textual description of the image.\n",
        "        \"\"\"\n",
        "        image = Image.open(image_path)\n",
        "        text = 'What are the objects present in the image please ignore the text? At the end give list of objects which are most highlighted in the image.'\n",
        "        # text = 'Please provide a detailed description of the objects present in the image while disregarding any textual elements. Afterward, list the objects that are most prominent or highlighted within the image. Ensure that the response follows this structure: first, describe the image, highlighting key features; then, provide a list of highlighted objects based on their significance or visibility in the visual context. USER:  <input> ASSISTANT: <description>List of objects:<list>'\n",
        "        prompt = f\"USER: <image>\\n{text}\\nASSISTANT:\"\n",
        "        outputs = self.pipe(image, prompt=prompt, generate_kwargs={\"max_new_tokens\": 200})\n",
        "        image_des = outputs[0][\"generated_text\"]\n",
        "        return image_des"
      ],
      "metadata": {
        "id": "mF24eDAB3f--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_image_description(image_description):\n",
        "    # Find the index of \"ASSISTANT\" in the string\n",
        "    assistant_index = image_description.find(\"ASSISTANT:\")\n",
        "    if assistant_index == -1:\n",
        "        return None, None\n",
        "\n",
        "    # Extract the text after \"ASSISTANT\"\n",
        "    assistant_text = image_description[assistant_index + len(\"ASSISTANT:\"):].strip()\n",
        "\n",
        "    # Split the text into lines\n",
        "    lines = assistant_text.split(\"\\n\")\n",
        "\n",
        "    # Find the index where the list of objects starts\n",
        "    objects_index = None\n",
        "    for i, line in enumerate(lines):\n",
        "        if \"List of objects:\" in line:\n",
        "            objects_index = i\n",
        "            break\n",
        "\n",
        "    if objects_index is None:\n",
        "        return assistant_text, None\n",
        "\n",
        "    # Extract the list of objects\n",
        "    objects_text = \"\\n\".join(lines[objects_index + 1:])\n",
        "\n",
        "    # Remove leading and trailing whitespaces\n",
        "    objects_text = objects_text.strip()\n",
        "\n",
        "    # Remove duplicates from the assistant_text\n",
        "    assistant_text_lines = assistant_text.split('\\n')\n",
        "    objects_lines = objects_text.split('\\n')\n",
        "\n",
        "    # Remove duplicates from the assistant_text\n",
        "    unique_lines = [line for line in assistant_text_lines if line not in objects_lines]\n",
        "\n",
        "    # Join the unique lines back into a string\n",
        "    unique_string = '\\n'.join(unique_lines)\n",
        "\n",
        "    return unique_string.strip().splitlines()[0], objects_text.strip()"
      ],
      "metadata": {
        "id": "kPsPY3KbLWiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
        "generator = ImageDescriptionGenerator(model_id)\n",
        "generator.load_model()"
      ],
      "metadata": {
        "id": "BgA4chAB42dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = '2.png'\n",
        "description = generator.generate_description(image_path)"
      ],
      "metadata": {
        "id": "Dn9GtNoT45Xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "description, objects = parse_image_description(description)\n",
        "print(description)\n",
        "print()\n",
        "print(objects)"
      ],
      "metadata": {
        "id": "j_OFfDX6IZIC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}